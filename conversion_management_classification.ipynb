{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ3dISqj3sys",
        "outputId": "d48babe3-18fb-4a9f-cf97-b88ebcfb1a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully installed groq and openai libraries.\n",
            "‚úÖ Groq API Key found in environment variables.\n",
            "‚úÖ Groq client initialized successfully and ready to use.\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: SETUP\n",
        "# This cell installs the necessary Python libraries and configures the API client.\n",
        "\n",
        "!pip install groq openai -q\n",
        "print(\"‚úÖ Successfully installed groq and openai libraries.\")\n",
        "\n",
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from getpass import getpass\n",
        "\n",
        "try:\n",
        "    api_key = os.environ[\"GROQ_API_KEY\"]\n",
        "    print(\"‚úÖ Groq API Key found in environment variables.\")\n",
        "except KeyError:\n",
        "    api_key = getpass(\"üîë Please enter your Groq API Key: \")\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = api_key\n",
        "\n",
        "try:\n",
        "    client = OpenAI(\n",
        "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        "        base_url=\"https://api.groq.com/openai/v1\",\n",
        "    )\n",
        "    print(\"‚úÖ Groq client initialized successfully and ready to use.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing Groq client: {e}\")\n",
        "    client = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: TASK 1 - CONVERSATION MANAGER\n",
        "# This cell defines the class that will manage our conversation history.\n",
        "# Running this cell will not produce any output, but it makes the class\n",
        "# available for the next cell to use.\n",
        "\n",
        "class ConversationManager:\n",
        "    \"\"\"\n",
        "    Manages conversation history with robust summarization and truncation.\n",
        "    \"\"\"\n",
        "    def __init__(self, client, model=\"llama-3.1-70b-versatile\", k_summarize=3):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "        self.k_summarize = k_summarize  # Summarize after every k-th run\n",
        "        self.turn_count = 0\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        \"\"\"Adds a message and triggers summarization if the condition is met.\"\"\"\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "        if role == \"user\":\n",
        "            self.turn_count += 1\n",
        "\n",
        "        # Check if it's time to summarize after a user's turn\n",
        "        if self.turn_count > 0 and self.turn_count % self.k_summarize == 0:\n",
        "            print(f\"\\n--- ‚ö†Ô∏è Triggering summarization after {self.turn_count} user turns. ---\")\n",
        "            self._summarize()\n",
        "\n",
        "    def _summarize(self):\n",
        "        \"\"\"Internal method to summarize the history using the Groq API.\"\"\"\n",
        "        # We don't summarize the very last message to maintain immediate context.\n",
        "        history_to_summarize = self.history[:-1]\n",
        "\n",
        "        if len(history_to_summarize) <= 1: # Only a system message\n",
        "            print(\"--- Not enough history to summarize. ---\")\n",
        "            return\n",
        "\n",
        "        prompt = (\n",
        "            \"Summarize the following conversation concisely. Capture key points, user intent, \"\n",
        "            \"and any important details mentioned. The summary will be used as context for a \"\n",
        "            \"new conversation. Do not add introductory phrases like 'The summary is...'.\\n\\n\"\n",
        "            f\"{json.dumps(history_to_summarize)}\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.1,\n",
        "            )\n",
        "            summary = response.choices[0].message.content.strip()\n",
        "            last_message = self.history[-1]\n",
        "\n",
        "            # Replace old history with a new system prompt containing the summary\n",
        "            self.history = [\n",
        "                {\"role\": \"system\", \"content\": f\"This is a summary of the previous conversation: {summary}\"},\n",
        "                last_message\n",
        "            ]\n",
        "            print(\"--- ‚úÖ Conversation summarized successfully. ---\")\n",
        "        except Exception as e:\n",
        "            print(f\"--- ‚ùå Error during summarization: {e} ---\")\n",
        "\n",
        "    def truncate_by_turns(self, n_turns):\n",
        "        \"\"\"Keeps only the last n conversation turns (1 turn = 1 user + 1 assistant).\"\"\"\n",
        "        num_messages = n_turns * 2\n",
        "        # Always keep the system message + the last n messages\n",
        "        self.history = [self.history[0]] + self.history[-num_messages:]\n",
        "        print(f\"\\n--- History truncated to the last {n_turns} turns. ---\")\n",
        "\n",
        "    def truncate_by_length(self, max_chars):\n",
        "        \"\"\"Keeps recent messages that fit within a specified character limit.\"\"\"\n",
        "        current_chars = 0\n",
        "        new_history = []\n",
        "        # Iterate backwards from the end, keeping messages until the limit is reached\n",
        "        for message in reversed(self.history[1:]): # Skip system message\n",
        "            current_chars += len(message[\"content\"])\n",
        "            if current_chars > max_chars:\n",
        "                break\n",
        "            new_history.insert(0, message)\n",
        "\n",
        "        self.history = [self.history[0]] + new_history\n",
        "        print(f\"\\n--- History truncated to fit under {max_chars} characters. ---\")\n",
        "\n",
        "    def print_history(self):\n",
        "        \"\"\"Prints the current history in a readable format.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìú CURRENT CONVERSATION HISTORY üìú\")\n",
        "        print(\"=\"*60)\n",
        "        for msg in self.history:\n",
        "            print(f\"[{msg['role'].upper()}]: {msg['content']}\")\n",
        "        print(\"=\"*60)"
      ],
      "metadata": {
        "id": "aOz2bVvv39p0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: TASK 1 - DEMONSTRATION\n",
        "# This cell demonstrates all functionalities of the ConversationManager class.\n",
        "\n",
        "if client:\n",
        "    print(\"üöÄ STARTING DEMONSTRATION FOR TASK 1: CONVERSATION MANAGEMENT üöÄ\")\n",
        "\n",
        "    print(\"\\n\\n--- 1. Testing Periodic Summarization (triggers on 3rd user message) ---\")\n",
        "    convo_manager = ConversationManager(client, k_summarize=3)\n",
        "\n",
        "    print(\"\\n--- Turn 1 ---\")\n",
        "    convo_manager.add_message(\"user\", \"Hi, I need help planning a trip to Japan.\")\n",
        "    convo_manager.add_message(\"assistant\", \"Of course! Japan is amazing. What cities are you interested in?\")\n",
        "    convo_manager.print_history()\n",
        "\n",
        "    print(\"\\n--- Turn 2 ---\")\n",
        "    convo_manager.add_message(\"user\", \"I want to see Tokyo for the city life and Kyoto for the culture.\")\n",
        "    convo_manager.add_message(\"assistant\", \"Great choices! How long will your trip be?\")\n",
        "    convo_manager.print_history()\n",
        "\n",
        "    print(\"\\n--- Turn 3 (Summarization should be triggered automatically after this) ---\")\n",
        "    convo_manager.add_message(\"user\", \"I'm planning for about 12 days in late October.\")\n",
        "    convo_manager.print_history()\n",
        "\n",
        "    print(\"\\n\\n--- 2. Testing Truncation by Number of Turns ---\")\n",
        "    trunc_manager_turns = ConversationManager(client)\n",
        "    trunc_manager_turns.add_message(\"user\", \"Message 1\"); trunc_manager_turns.add_message(\"assistant\", \"Response 1\")\n",
        "    trunc_manager_turns.add_message(\"user\", \"Message 2\"); trunc_manager_turns.add_message(\"assistant\", \"Response 2\")\n",
        "    trunc_manager_turns.add_message(\"user\", \"Message 3\"); trunc_manager_turns.add_message(\"assistant\", \"Response 3\")\n",
        "    print(\"\\n--- Full History Before Truncation ---\")\n",
        "    trunc_manager_turns.print_history()\n",
        "    trunc_manager_turns.truncate_by_turns(n_turns=1)\n",
        "    trunc_manager_turns.print_history()\n",
        "\n",
        "    print(\"\\n\\n--- 3. Testing Truncation by Character Length ---\")\n",
        "    trunc_manager_len = ConversationManager(client)\n",
        "    trunc_manager_len.add_message(\"user\", \"This is a very long initial user message designed specifically to test the character truncation feature.\")\n",
        "    trunc_manager_len.add_message(\"assistant\", \"This is an equally verbose assistant response to add more length to the conversation history.\")\n",
        "    trunc_manager_len.add_message(\"user\", \"This is a shorter, more recent message.\")\n",
        "    trunc_manager_len.add_message(\"assistant\", \"A short reply.\")\n",
        "    print(\"\\n--- Full History Before Truncation ---\")\n",
        "    trunc_manager_len.print_history()\n",
        "    trunc_manager_len.truncate_by_length(max_chars=100)\n",
        "    trunc_manager_len.print_history()\n",
        "else:\n",
        "    print(\"Client not initialized. Please run Cell 1 and provide your API key.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwDhKXMK4Y0U",
        "outputId": "9b2f1da9-41ec-4181-e4dc-d09d840a2fa7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ STARTING DEMONSTRATION FOR TASK 1: CONVERSATION MANAGEMENT üöÄ\n",
            "\n",
            "\n",
            "--- 1. Testing Periodic Summarization (triggers on 3rd user message) ---\n",
            "\n",
            "--- Turn 1 ---\n",
            "\n",
            "============================================================\n",
            "üìú CURRENT CONVERSATION HISTORY üìú\n",
            "============================================================\n",
            "[SYSTEM]: You are a helpful assistant.\n",
            "[USER]: Hi, I need help planning a trip to Japan.\n",
            "[ASSISTANT]: Of course! Japan is amazing. What cities are you interested in?\n",
            "============================================================\n",
            "\n",
            "--- Turn 2 ---\n",
            "\n",
            "============================================================\n",
            "üìú CURRENT CONVERSATION HISTORY üìú\n",
            "============================================================\n",
            "[SYSTEM]: You are a helpful assistant.\n",
            "[USER]: Hi, I need help planning a trip to Japan.\n",
            "[ASSISTANT]: Of course! Japan is amazing. What cities are you interested in?\n",
            "[USER]: I want to see Tokyo for the city life and Kyoto for the culture.\n",
            "[ASSISTANT]: Great choices! How long will your trip be?\n",
            "============================================================\n",
            "\n",
            "--- Turn 3 (Summarization should be triggered automatically after this) ---\n",
            "\n",
            "--- ‚ö†Ô∏è Triggering summarization after 3 user turns. ---\n",
            "--- ‚ùå Error during summarization: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}} ---\n",
            "\n",
            "============================================================\n",
            "üìú CURRENT CONVERSATION HISTORY üìú\n",
            "============================================================\n",
            "[SYSTEM]: You are a helpful assistant.\n",
            "[USER]: Hi, I need help planning a trip to Japan.\n",
            "[ASSISTANT]: Of course! Japan is amazing. What cities are you interested in?\n",
            "[USER]: I want to see Tokyo for the city life and Kyoto for the culture.\n",
            "[ASSISTANT]: Great choices! How long will your trip be?\n",
            "[USER]: I'm planning for about 12 days in late October.\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- 2. Testing Truncation by Number of Turns ---\n",
            "\n",
            "--- ‚ö†Ô∏è Triggering summarization after 3 user turns. ---\n",
            "--- ‚ùå Error during summarization: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}} ---\n",
            "\n",
            "--- ‚ö†Ô∏è Triggering summarization after 3 user turns. ---\n",
            "--- ‚ùå Error during summarization: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}} ---\n",
            "\n",
            "--- Full History Before Truncation ---\n",
            "\n",
            "============================================================\n",
            "üìú CURRENT CONVERSATION HISTORY üìú\n",
            "============================================================\n",
            "[SYSTEM]: You are a helpful assistant.\n",
            "[USER]: Message 1\n",
            "[ASSISTANT]: Response 1\n",
            "[USER]: Message 2\n",
            "[ASSISTANT]: Response 2\n",
            "[USER]: Message 3\n",
            "[ASSISTANT]: Response 3\n",
            "============================================================\n",
            "\n",
            "--- History truncated to the last 1 turns. ---\n",
            "\n",
            "============================================================\n",
            "üìú CURRENT CONVERSATION HISTORY üìú\n",
            "============================================================\n",
            "[SYSTEM]: You are a helpful assistant.\n",
            "[USER]: Message 3\n",
            "[ASSISTANT]: Response 3\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- 3. Testing Truncation by Character Length ---\n",
            "\n",
            "--- Full History Before Truncation ---\n",
            "\n",
            "============================================================\n",
            "üìú CURRENT CONVERSATION HISTORY üìú\n",
            "============================================================\n",
            "[SYSTEM]: You are a helpful assistant.\n",
            "[USER]: This is a very long initial user message designed specifically to test the character truncation feature.\n",
            "[ASSISTANT]: This is an equally verbose assistant response to add more length to the conversation history.\n",
            "[USER]: This is a shorter, more recent message.\n",
            "[ASSISTANT]: A short reply.\n",
            "============================================================\n",
            "\n",
            "--- History truncated to fit under 100 characters. ---\n",
            "\n",
            "============================================================\n",
            "üìú CURRENT CONVERSATION HISTORY üìú\n",
            "============================================================\n",
            "[SYSTEM]: You are a helpful assistant.\n",
            "[USER]: This is a shorter, more recent message.\n",
            "[ASSISTANT]: A short reply.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: TASK 2 - JSON EXTRACTION LOGIC\n",
        "# This cell defines the schema for data extraction and the function that\n",
        "# calls the Groq API using its 'tool calling' feature. Running it will\n",
        "# not produce output but makes the function ready for the next cell.\n",
        "\n",
        "user_details_schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"extract_user_info\",\n",
        "        \"description\": \"Extracts user information from a chat message.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The user's full name.\"},\n",
        "                \"email\": {\"type\": \"string\", \"description\": \"The user's email address.\"},\n",
        "                \"phone\": {\"type\": \"string\", \"description\": \"The user's phone number.\"},\n",
        "                \"location\": {\"type\": \"string\", \"description\": \"The user's city or address.\"},\n",
        "                \"age\": {\"type\": \"integer\", \"description\": \"The user's age in years.\"},\n",
        "            },\n",
        "            \"required\": []\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def extract_info_from_chat(chat_text, client, model=\"llama-3.1-70b-versatile\"):\n",
        "    \"\"\"\n",
        "    Uses Groq's tool calling feature to extract structured information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert information extraction system. Your task is to extract user details from the provided text and format it using the 'extract_user_info' tool. If a detail is not present, do not include it in the output.\"},\n",
        "                {\"role\": \"user\", \"content\": chat_text}\n",
        "            ],\n",
        "            tools=[user_details_schema],\n",
        "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_user_info\"}},\n",
        "            temperature=0.0,\n",
        "        )\n",
        "\n",
        "        tool_call = response.choices[0].message.tool_calls[0]\n",
        "        arguments_str = tool_call.function.arguments\n",
        "\n",
        "        extracted_data = json.loads(arguments_str)\n",
        "        return extracted_data\n",
        "    except (json.JSONDecodeError, IndexError, AttributeError) as e:\n",
        "        return {\"error\": \"Failed to extract or parse valid data.\", \"details\": str(e)}\n",
        "    except Exception as e:\n",
        "        return {\"error\": \"An unexpected API error occurred.\", \"details\": str(e)}"
      ],
      "metadata": {
        "id": "VPG_idx84llP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: TASK 2 - DEMONSTRATION\n",
        "# This cell runs the demonstration for the JSON extraction function,\n",
        "# processing several sample chats and showing the structured output.\n",
        "\n",
        "if client:\n",
        "    print(\"üöÄ STARTING DEMONSTRATION FOR TASK 2: JSON EXTRACTION üöÄ\")\n",
        "\n",
        "    chat_samples = [\n",
        "        \"Hi, my name is John Doe. I'm 29 years old and I live in New York. You can reach me at john.doe@email.com or on my mobile at 555-123-4567.\",\n",
        "        \"Please sign me up for the newsletter. My name is Jane Smith and my email is jane.s@web.com. My phone is 555-222-3333.\",\n",
        "        \"hey can you help me? im david, my e-mail is dave.c@mail.net and my fone is 555-987-6543. i'm based in Chicago\",\n",
        "        \"My phone number is 555-555-5555. My name is Alex Ray. I am 42 years old. My email address is alex.ray@work.org.\"\n",
        "    ]\n",
        "\n",
        "    for i, sample in enumerate(chat_samples):\n",
        "        print(f\"\\n\\n--- üìù Processing Sample {i+1} ---\")\n",
        "        print(f\"Input Text: \\\"{sample}\\\"\")\n",
        "\n",
        "        extracted_json = extract_info_from_chat(sample, client)\n",
        "\n",
        "        print(\"\\n‚úÖ Extracted Information (Validated JSON):\")\n",
        "        print(json.dumps(extracted_json, indent=2))\n",
        "else:\n",
        "    print(\"Client not initialized. Please run Cell 1 and provide your API key.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L_Fusah4ox-",
        "outputId": "1557e790-eb7b-451d-971a-617623023d93"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ STARTING DEMONSTRATION FOR TASK 2: JSON EXTRACTION üöÄ\n",
            "\n",
            "\n",
            "--- üìù Processing Sample 1 ---\n",
            "Input Text: \"Hi, my name is John Doe. I'm 29 years old and I live in New York. You can reach me at john.doe@email.com or on my mobile at 555-123-4567.\"\n",
            "\n",
            "‚úÖ Extracted Information (Validated JSON):\n",
            "{\n",
            "  \"error\": \"An unexpected API error occurred.\",\n",
            "  \"details\": \"Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\"\n",
            "}\n",
            "\n",
            "\n",
            "--- üìù Processing Sample 2 ---\n",
            "Input Text: \"Please sign me up for the newsletter. My name is Jane Smith and my email is jane.s@web.com. My phone is 555-222-3333.\"\n",
            "\n",
            "‚úÖ Extracted Information (Validated JSON):\n",
            "{\n",
            "  \"error\": \"An unexpected API error occurred.\",\n",
            "  \"details\": \"Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\"\n",
            "}\n",
            "\n",
            "\n",
            "--- üìù Processing Sample 3 ---\n",
            "Input Text: \"hey can you help me? im david, my e-mail is dave.c@mail.net and my fone is 555-987-6543. i'm based in Chicago\"\n",
            "\n",
            "‚úÖ Extracted Information (Validated JSON):\n",
            "{\n",
            "  \"error\": \"An unexpected API error occurred.\",\n",
            "  \"details\": \"Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\"\n",
            "}\n",
            "\n",
            "\n",
            "--- üìù Processing Sample 4 ---\n",
            "Input Text: \"My phone number is 555-555-5555. My name is Alex Ray. I am 42 years old. My email address is alex.ray@work.org.\"\n",
            "\n",
            "‚úÖ Extracted Information (Validated JSON):\n",
            "{\n",
            "  \"error\": \"An unexpected API error occurred.\",\n",
            "  \"details\": \"Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YnhsSPUS4ucI"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}